{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ssaba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ssaba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\ssaba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\ssaba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "from transformers import MarianMTModel, MarianTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data_path = './data/dataset.xlsx'):\n",
    "    data = pd.read_excel(data_path)\n",
    "    clean_data = data.drop(columns =\n",
    "                 ['Unnamed: 0', 'contrast',\t'goal', 'goals2',\t'list', 'metaphor',\t'moral',\n",
    "                  'question','collective','story','date','q9video','sexd3','sexd1','sexd2',\n",
    "                  'charisma','_merge','prolific_score','prolific_indicator_all',\n",
    "                  'text_length_all', 'sex'], inplace = False)\n",
    "    clean_data.dropna(inplace=True)\n",
    "    \n",
    "    return clean_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sentences</th>\n",
       "      <th>words</th>\n",
       "      <th>num_approvals</th>\n",
       "      <th>num_rejections</th>\n",
       "      <th>education</th>\n",
       "      <th>hones16</th>\n",
       "      <th>emoti16</th>\n",
       "      <th>extra16</th>\n",
       "      <th>agree16</th>\n",
       "      <th>...</th>\n",
       "      <th>icar_hat0</th>\n",
       "      <th>icar_hat1</th>\n",
       "      <th>icar_hat2</th>\n",
       "      <th>final_text</th>\n",
       "      <th>overall_sentiment_all</th>\n",
       "      <th>positive_sentiment_all</th>\n",
       "      <th>negative_sentiment_all</th>\n",
       "      <th>neutra_sentiment_all</th>\n",
       "      <th>mixed_sentiment_all</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5e1cf0eb65b6d3071f489de9</td>\n",
       "      <td>35</td>\n",
       "      <td>771</td>\n",
       "      <td>534</td>\n",
       "      <td>2</td>\n",
       "      <td>Master's degree (e.g. MA, MS, MEd)</td>\n",
       "      <td>3.0417</td>\n",
       "      <td>3.6667</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.068836</td>\n",
       "      <td>9.389475</td>\n",
       "      <td>9.459121</td>\n",
       "      <td>Hello everyone. Thank you. Taking the time to ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.9569</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55d06fd334e9060012e5781c</td>\n",
       "      <td>39</td>\n",
       "      <td>424</td>\n",
       "      <td>972</td>\n",
       "      <td>7</td>\n",
       "      <td>Bachelor's degree (e.g. BA, BS)</td>\n",
       "      <td>3.3750</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>2.7500</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.440331</td>\n",
       "      <td>8.776836</td>\n",
       "      <td>8.818454</td>\n",
       "      <td>Hi, I am Kathy. I'd love to be considered for ...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>MED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>615586b009f801c3f2d4af8d</td>\n",
       "      <td>18</td>\n",
       "      <td>449</td>\n",
       "      <td>311</td>\n",
       "      <td>2</td>\n",
       "      <td>Master's degree (e.g. MA, MS, MEd)</td>\n",
       "      <td>3.4167</td>\n",
       "      <td>3.2917</td>\n",
       "      <td>3.4583</td>\n",
       "      <td>3.4167</td>\n",
       "      <td>...</td>\n",
       "      <td>8.754018</td>\n",
       "      <td>9.077301</td>\n",
       "      <td>8.959213</td>\n",
       "      <td>uh yeah I I think I would be the best candidat...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.1747</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>MED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5847e60f73170700013697c6</td>\n",
       "      <td>21</td>\n",
       "      <td>611</td>\n",
       "      <td>763</td>\n",
       "      <td>2</td>\n",
       "      <td>Bachelor's degree (e.g. BA, BS)</td>\n",
       "      <td>4.2083</td>\n",
       "      <td>1.8750</td>\n",
       "      <td>2.6250</td>\n",
       "      <td>3.7083</td>\n",
       "      <td>...</td>\n",
       "      <td>9.392123</td>\n",
       "      <td>9.710048</td>\n",
       "      <td>9.777639</td>\n",
       "      <td>Hello. Um I've of course a fair amount of expe...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.5761</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.2484</td>\n",
       "      <td>0.0570</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6086a11397234e7f83e4e793</td>\n",
       "      <td>13</td>\n",
       "      <td>611</td>\n",
       "      <td>304</td>\n",
       "      <td>0</td>\n",
       "      <td>Doctorate or professional degree (e.g. MD, DDS...</td>\n",
       "      <td>2.8750</td>\n",
       "      <td>2.5833</td>\n",
       "      <td>4.1667</td>\n",
       "      <td>2.9583</td>\n",
       "      <td>...</td>\n",
       "      <td>9.242874</td>\n",
       "      <td>9.557817</td>\n",
       "      <td>9.304668</td>\n",
       "      <td>Okay, so I would like to thank you for giving ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.8515</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.1456</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>HIGH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             participant_id  sentences  words  num_approvals  num_rejections  \\\n",
       "0  5e1cf0eb65b6d3071f489de9         35    771            534               2   \n",
       "1  55d06fd334e9060012e5781c         39    424            972               7   \n",
       "2  615586b009f801c3f2d4af8d         18    449            311               2   \n",
       "3  5847e60f73170700013697c6         21    611            763               2   \n",
       "4  6086a11397234e7f83e4e793         13    611            304               0   \n",
       "\n",
       "                                           education  hones16  emoti16  \\\n",
       "0                 Master's degree (e.g. MA, MS, MEd)   3.0417   3.6667   \n",
       "1                    Bachelor's degree (e.g. BA, BS)   3.3750   3.5000   \n",
       "2                 Master's degree (e.g. MA, MS, MEd)   3.4167   3.2917   \n",
       "3                    Bachelor's degree (e.g. BA, BS)   4.2083   1.8750   \n",
       "4  Doctorate or professional degree (e.g. MD, DDS...   2.8750   2.5833   \n",
       "\n",
       "   extra16  agree16  ...  icar_hat0  icar_hat1  icar_hat2  \\\n",
       "0   2.0000   3.5000  ...   9.068836   9.389475   9.459121   \n",
       "1   2.7500   3.0000  ...   8.440331   8.776836   8.818454   \n",
       "2   3.4583   3.4167  ...   8.754018   9.077301   8.959213   \n",
       "3   2.6250   3.7083  ...   9.392123   9.710048   9.777639   \n",
       "4   4.1667   2.9583  ...   9.242874   9.557817   9.304668   \n",
       "\n",
       "                                          final_text overall_sentiment_all  \\\n",
       "0  Hello everyone. Thank you. Taking the time to ...              POSITIVE   \n",
       "1  Hi, I am Kathy. I'd love to be considered for ...               NEUTRAL   \n",
       "2  uh yeah I I think I would be the best candidat...              POSITIVE   \n",
       "3  Hello. Um I've of course a fair amount of expe...              POSITIVE   \n",
       "4  Okay, so I would like to thank you for giving ...              POSITIVE   \n",
       "\n",
       "  positive_sentiment_all negative_sentiment_all neutra_sentiment_all  \\\n",
       "0                 0.9569                 0.0007               0.0417   \n",
       "1                 0.1587                 0.0055               0.8350   \n",
       "2                 0.8051                 0.0164               0.1747   \n",
       "3                 0.5761                 0.1185               0.2484   \n",
       "4                 0.8515                 0.0016               0.1456   \n",
       "\n",
       "   mixed_sentiment_all  targets  \n",
       "0               0.0007     HIGH  \n",
       "1               0.0009      MED  \n",
       "2               0.0039      MED  \n",
       "3               0.0570     HIGH  \n",
       "4               0.0013     HIGH  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = clean_data()\n",
    "data2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1\n",
    "def back_translation(text, source_language = 'english', target_language = 'french'):\n",
    "    temperature = 1\n",
    "    original_text = text\n",
    "    tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "    model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    sentences = sent_tokenize(original_text)\n",
    "    back_translated_sentences = []\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        translated_ids = model.generate(**inputs, do_sample=True, temperature=temperature, max_length=1024)\n",
    "        intermediate_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "        back_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "        back_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\").to(device)\n",
    "        back_inputs = back_tokenizer(intermediate_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        back_translated_ids = back_model.generate(**back_inputs, do_sample=True, temperature=temperature,max_length=1024)\n",
    "        back_translated_text = back_tokenizer.decode(back_translated_ids[0], skip_special_tokens=True)\n",
    "        back_translated_sentences.append(back_translated_text)\n",
    "\n",
    "    final_text = \" \".join(back_translated_sentences)\n",
    "    \n",
    "    \n",
    "    return final_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "def noise_injection(text, char_insert_p = 0.2, ocr_aug_p = 0.1, word_swaping_aug_p = 0.3):\n",
    "    \n",
    "    word_swaping_aug = naw.RandomWordAug(action = \"swap\", aug_p = word_swaping_aug_p)\n",
    "    text = word_swaping_aug.augment(text)\n",
    "    \n",
    "    char_aug = nac.RandomCharAug(action = \"insert\", aug_char_p = char_insert_p)\n",
    "    text = char_aug.augment(text)\n",
    "\n",
    "    ocr_aug = nac.OcrAug(aug_char_p = ocr_aug_p)\n",
    "    text = ocr_aug.augment(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3\n",
    "def synonyme_replacement_tfidf_dropout(text, tfidf_scores, dropout_p=0.5, replace_p=0.5):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    augmented_words = []\n",
    "\n",
    "    for i, (word, pos) in enumerate(pos_tags):\n",
    "        word_lower = word.lower()\n",
    "\n",
    "        if word_lower in nltk.corpus.stopwords.words('english') or not word.isalpha():\n",
    "            augmented_words.append(word)\n",
    "            continue\n",
    "\n",
    "        tfidf_score = tfidf_scores.get(word_lower, min(tfidf_scores.values()))\n",
    "        max_tfidf = max(tfidf_scores.values())\n",
    "        min_tfidf = min(tfidf_scores.values())\n",
    "        normalized_tfidf = (tfidf_score - min_tfidf) / (max_tfidf - min_tfidf)\n",
    "\n",
    "        dropout_prob = dropout_p * (1 - normalized_tfidf)\n",
    "        replace_prob = replace_p * (1 - normalized_tfidf)\n",
    "\n",
    "        p = random.random()\n",
    "\n",
    "        if p < dropout_prob:\n",
    "            continue\n",
    "\n",
    "        elif p < dropout_prob + replace_prob:\n",
    "\n",
    "            word_pos = get_wordnet_pos(pos)\n",
    "            if word_pos is None:\n",
    "                augmented_words.append(word)\n",
    "                continue\n",
    "\n",
    "            synonyms = []\n",
    "            for syn in wordnet.synsets(word, pos=word_pos):\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonym = lemma.name().replace('_', ' ')\n",
    "                    if synonym.lower() != word_lower:\n",
    "                        synonyms.append(synonym)\n",
    "\n",
    "            if synonyms:\n",
    "                syn_word = random.choice(synonyms)\n",
    "                augmented_words.append(syn_word)\n",
    "\n",
    "            else:\n",
    "                augmented_words.append(word)\n",
    "\n",
    "        else:\n",
    "            augmented_words.append(word)\n",
    "            \n",
    "    augmented_text = ' '.join(augmented_words)\n",
    "    return augmented_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(corpus_type = 'reuters'):\n",
    "    if corpus_type == 'reuters':\n",
    "        sentences = reuters.sents()\n",
    "        corpus = [' '.join(sentence) for sentence in sentences]\n",
    "        return corpus\n",
    "    \n",
    "    if corpus_type == 'brown':\n",
    "        sentences = brown.sents()\n",
    "        corpus = [' '.join(sentence) for sentence in sentences]\n",
    "        return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    tfidf_scores = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "    return tfidf_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_by_tfidf_pos(text, tfidf_scores, mask_p = 0.3, target_pos=['NN', 'VB', 'JJ', 'RB']):\n",
    "   \n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "    masked_tokens = words.copy()\n",
    "    max_tfidf = max(tfidf_scores.values())\n",
    "    min_tfidf = min(tfidf_scores.values())\n",
    "\n",
    "    for i, (word, pos) in enumerate(pos_tags):\n",
    "        word_lower = word.lower()\n",
    "        if not word.isalpha() or word_lower not in tfidf_scores:\n",
    "            continue\n",
    "\n",
    "        if not any(pos.startswith(target) for target in target_pos):\n",
    "            continue\n",
    "\n",
    "        tfidf = tfidf_scores[word_lower]\n",
    "        normalized_tfidf = (tfidf - min_tfidf) / (max_tfidf - min_tfidf)\n",
    "        mask_prob = mask_p * (1 - normalized_tfidf)\n",
    "        p = random.random()\n",
    "\n",
    "        if p < mask_prob:\n",
    "            masked_tokens[i] = '[MASK]'\n",
    "\n",
    "    return masked_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4\n",
    "def contexual_bert_by_tfidf_pos(text, tfidf_scores, mask_p = 0.3, n_augments = 1, target_pos=['NN', 'VB', 'JJ', 'RB'], model_name = 'bert-base-uncased'):\n",
    "\n",
    "    \n",
    "    augmented_texts = []\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    config = model.config\n",
    "    config.max_position_embeddings = 1024  \n",
    "    model = BertForMaskedLM(config)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for _ in range(n_augments):\n",
    "        masked_tokens = mask_by_tfidf_pos(text, tfidf_scores, mask_p, target_pos)\n",
    "        masked_ids = tokenizer.convert_tokens_to_ids(masked_tokens)\n",
    "        input_ids = torch.tensor([masked_ids]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            predictions = outputs.logits\n",
    "\n",
    "        predicted_tokens = masked_tokens.copy()\n",
    "        for i, token in enumerate(masked_tokens):\n",
    "            if token == '[MASK]':\n",
    "                predicted_id = torch.argmax(predictions[0, i]).item()\n",
    "                predicted_token = tokenizer.convert_ids_to_tokens([predicted_id])[0]\n",
    "                predicted_tokens[i] = predicted_token\n",
    "\n",
    "        augmented_text = tokenizer.convert_tokens_to_string(predicted_tokens)\n",
    "        augmented_texts.append(augmented_text)\n",
    "\n",
    "    return augmented_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(data, translation = False, source_language = 'english', target_language = 'french',\n",
    "noise = True, char_insert_p = 0.2, ocr_aug_p = 0.1, word_swaping_aug_p = 0.2,\n",
    "dropout_p=0.3, replace_p=0.5,\n",
    "mask_p=0.5, n_augments=1, target_corpus = 'reuters'):\n",
    "    \n",
    "    new_data = pd.DataFrame(columns = data.columns)\n",
    "    corpus = get_corpus(corpus_type = target_corpus)\n",
    "    tfidf_scores = compute_tfidf(corpus)    \n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        text = data.iloc[i,:]['final_text']\n",
    "        for j in range(n_augments):\n",
    "            p = random.random()\n",
    "            new_text = \"\"\n",
    "            if p < 0.3:\n",
    "                new_text = text\n",
    "            \n",
    "                #if translation:\n",
    "                new_text = back_translation(new_text, source_language, target_language)\n",
    "\n",
    "                #new_text =synonyme_replacement_tfidf_dropout(new_text, tfidf_scores, dropout_p = dropout_p, replace_p = replace_p)                \n",
    "                #new_text = contexual_bert_by_tfidf_pos(new_text, tfidf_scores, mask_p = mask_p, n_augments = 1, target_pos=['NN', 'VB', 'JJ', 'RB'])[0]\n",
    "\n",
    "                #if noise:\n",
    "                new_text = noise_injection(text = new_text, char_insert_p= char_insert_p, ocr_aug_p= ocr_aug_p, word_swaping_aug_p = word_swaping_aug_p)[0]\n",
    "\n",
    "                new_data.loc[len(new_data)] = data.iloc[i].copy()\n",
    "                new_data.loc[len(new_data)-1, 'final_text'] = new_text\n",
    "                \n",
    "            else:\n",
    "                new_text = text\n",
    "            \n",
    "                new_text =synonyme_replacement_tfidf_dropout(new_text, tfidf_scores, dropout_p = dropout_p, replace_p = replace_p)                \n",
    "                new_text = contexual_bert_by_tfidf_pos(new_text, tfidf_scores, mask_p = mask_p, n_augments = 1, target_pos=['NN', 'VB', 'JJ', 'RB'])[0]\n",
    "\n",
    "                #if noise:\n",
    "                new_text = noise_injection(text = new_text, char_insert_p= char_insert_p, ocr_aug_p= ocr_aug_p, word_swaping_aug_p = word_swaping_aug_p)[0]\n",
    "\n",
    "                new_data.loc[len(new_data)] = data.iloc[i].copy()\n",
    "                new_data.loc[len(new_data)-1, 'final_text'] = new_text\n",
    "                \n",
    "            new_data.loc[len(new_data)-1, 'participant_id'] = data.iloc[i]['participant_id']\n",
    "            new_data.loc[len(new_data)-1, 'sentences'] = len(sent_tokenize(new_text))\n",
    "            new_data.loc[len(new_data)-1, 'words'] = len(new_text.split())       \n",
    "            new_data.loc[len(new_data)-1, 'num_approvals'] = data.iloc[i]['num_approvals']\n",
    "            new_data.loc[len(new_data)-1, 'num_rejections'] = data.iloc[i]['num_rejections']\n",
    "            new_data.loc[len(new_data)-1, 'education'] = data.iloc[i]['education']    \n",
    "            new_data.loc[len(new_data)-1, 'hones16'] = np.clip(data.iloc[i]['hones16'] + np.random.normal(0, 0.1), 0, 5)\n",
    "            new_data.loc[len(new_data)-1, 'emoti16'] = np.clip(data.iloc[i]['emoti16'] + np.random.normal(0, 0.1), 0, 5)\n",
    "            new_data.loc[len(new_data)-1, 'extra16'] = np.clip(data.iloc[i]['extra16'] + np.random.normal(0, 0.1), 0, 5)\n",
    "            new_data.loc[len(new_data)-1, 'agree16'] = np.clip(data.iloc[i]['agree16'] + np.random.normal(0, 0.1), 0, 5)\n",
    "            new_data.loc[len(new_data)-1,'consc16'] = np.clip(data.iloc[i]['consc16'] + np.random.normal(0, 0.1), 0, 5)\n",
    "            new_data.loc[len(new_data)-1, 'openn16'] = np.clip(data.iloc[i]['openn16'] + np.random.normal(0, 0.1), 0, 5)\n",
    "            new_data.loc[len(new_data)-1, 'time_taken'] = data.iloc[i]['time_taken'] + np.random.normal(0, 2)\n",
    "            new_data.loc[len(new_data)-1, 'age'] = data.iloc[i]['age'] + int(np.round(np.random.normal(0, 1)))\n",
    "            new_data.loc[len(new_data)-1, 'gender'] = data.iloc[i]['gender']\n",
    "            new_data.loc[len(new_data)-1, 'ethnicity'] = data.iloc[i]['ethnicity']\n",
    "            new_data.loc[len(new_data)-1, 'employment'] = data.iloc[i]['employment']\n",
    "            new_data.loc[len(new_data)-1, 'status'] = data.iloc[i]['status']\n",
    "            new_data.loc[len(new_data)-1, 'incentive'] = data.iloc[i]['incentive']\n",
    "            new_data.loc[len(new_data)-1, 'icar'] = data.iloc[i]['icar'] + np.random.normal(0, 0.1)\n",
    "            new_data.loc[len(new_data)-1, 'icar_hat0'] = data.iloc[i]['icar_hat0'] + np.random.normal(0, 0.1)\n",
    "            new_data.loc[len(new_data)-1, 'icar_hat1'] = data.iloc[i]['icar_hat1'] + np.random.normal(0, 0.1)\n",
    "            new_data.loc[len(new_data)-1, 'icar_hat2'] = data.iloc[i]['icar_hat2'] + np.random.normal(0, 0.1)\n",
    "            new_data.loc[len(new_data)-1, 'overall_sentiment_all'] = data.iloc[i]['overall_sentiment_all']\n",
    "            new_data.loc[len(new_data)-1, 'positive_sentiment_all'] = data.iloc[i]['positive_sentiment_all'] + np.random.normal(0, 0.1)\n",
    "            new_data.loc[len(new_data)-1, 'negative_sentiment_all'] = data.iloc[i]['negative_sentiment_all'] + np.random.normal(0, 0.1)\n",
    "            new_data.loc[len(new_data)-1, 'neutra_sentiment_all'] = data.iloc[i]['neutra_sentiment_all'] + np.random.normal(0, 0.1)\n",
    "            new_data.loc[len(new_data)-1, 'mixed_sentiment_all'] = data.iloc[i]['mixed_sentiment_all'] + np.random.normal(0, 0.1)\n",
    "            new_data.loc[len(new_data)-1, 'targets'] = data.iloc[i]['targets']    \n",
    "\n",
    "\n",
    "\n",
    "    dummy_new_data = pd.get_dummies(new_data, columns=['education', 'gender', 'ethnicity', 'employment', 'status', 'incentive', 'overall_sentiment_all' , 'targets'], drop_first=True, dtype=int)\n",
    "    return dummy_new_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data =data_augmentation(data = data2.iloc[:10], n_augments= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
